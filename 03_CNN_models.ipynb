{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import models, layers, losses, metrics, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 튜토리얼에서는 CNN 모델을 빌드하는 법을 알아보겠습니다. 이전 튜토리얼에서는 sequential()로 빠르게 모델을 구성해봤는데, 이제부터 저만의 정석(?)대로 모델을 구성할 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 연습용 MNIST 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 연습용으로 쓸 MNIST 데이터를 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets.mnist import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = trainset\n",
    "x_test, y_test = testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n",
      "(10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(*x_train.shape, 1)\n",
    "x_test = x_test.reshape(*x_test.shape, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 간단히 데이터를 standardize하는 과정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean = 0, std = 0.5\n",
    "x_train = (x_train.astype(np.float32) - 128) / 256\n",
    "x_test = (x_test.astype(np.float32) - 128) / 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로는, 학습할 때 사용할 몇 가지 헬퍼 함수를 정의해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(x, y):\n",
    "    \"\"\"\n",
    "    x, y를 셔플한다.\n",
    "    \n",
    "    Arguments:\n",
    "    ----------\n",
    "    x : features 데이터 행렬 (N, ...)\n",
    "    y : 라벨 벡터            (N,)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    x[r] : x를 셔플한 np.array (N, ...)\n",
    "    y[r] : y를 셔플한 np.array (N,)\n",
    "    \"\"\"\n",
    "    \n",
    "    n = x.shape[0]\n",
    "    \n",
    "    r = np.arange(n)\n",
    "    np.random.shuffle(r)\n",
    "    \n",
    "    return x[r], y[r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(x, y, batch_size):\n",
    "    \"\"\"\n",
    "    x, y를 해당 batch_size만큼 잘라서 배치를 생성해주는 generator\n",
    "    \n",
    "    Arguments:\n",
    "    ----------\n",
    "    x : features 데이터 행렬 (N, ...)\n",
    "    y : 라벨 벡터            (N,)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    x_batch : x를 배치단위로 자른 것\n",
    "    y_batch : y를 배치단위로 자른 것.\n",
    "    \"\"\"\n",
    "    \n",
    "    n = x.shape[0]\n",
    "    n_batches = int(np.ceil(n / batch_size))\n",
    "    \n",
    "    for b in range(n_batches):\n",
    "        start = b*batch_size\n",
    "        end = min(n, (b+1)*batch_size)\n",
    "        \n",
    "        yield x[start:end], y[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자, 이제 모델을 구성할 것입니다.\n",
    "\n",
    "제가 좋아하는 방식은 바로 subclassing 방식인데요, keras의 모델 클래스를 상속받아 저만의 모델을 만드는 것입니다! 왠지 객체지향적인 것 같아서 좋더라구요..\n",
    "어쨌든, CNN 모델을 구성해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNNModel(models.Model):\n",
    "    \"\"\"\n",
    "    My CNN Model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MyCNNModel, self).__init__() # 반드시 생성자 맨 처음에 호출해 줘야 함.\n",
    "        \n",
    "        # convolution - pool - convolution - pool\n",
    "        self.features = models.Sequential([\n",
    "            layers.Conv2D(8, (3, 3), strides=1, padding=\"same\", input_shape=(28, 28, 1)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Activation(tf.nn.tanh),\n",
    "            \n",
    "            layers.MaxPool2D((2, 2), strides=2, padding=\"same\"),\n",
    "            \n",
    "            layers.Conv2D(12, (3, 3), strides=1, padding=\"same\"),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Activation(tf.nn.tanh),\n",
    "            \n",
    "            layers.MaxPool2D((2, 2), strides=2, padding=\"same\"),\n",
    "        ])\n",
    "        \n",
    "        # dense - dense - dense\n",
    "        self.classifier = models.Sequential([\n",
    "            layers.Dense(64, input_shape=(7*7*12,)),\n",
    "            layers.Activation(tf.nn.tanh),\n",
    "            layers.Dropout(0.5),\n",
    "            \n",
    "            layers.Dense(32),\n",
    "            layers.Activation(tf.nn.tanh),\n",
    "            layers.Dropout(0.5),\n",
    "            \n",
    "            layers.Dense(10),\n",
    "            layers.Activation(tf.nn.softmax)\n",
    "        ])\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs, training=False):\n",
    "        '''\n",
    "        forward propagation function\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "        inputs : input tensor. (batch_size, 28, 28, 1)\n",
    "        training: 트레이닝 과정이면 True 넣어줘야 함\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        preds : 추론 결과\n",
    "        '''\n",
    "        \n",
    "        # 하위 모델들도 본 모델과 같은 training을 적용시켜 준다.\n",
    "        # (특히 dropout이 있는 경우 training=False일때 drop_rate = 0.0이 됨)\n",
    "        self.features.training = training\n",
    "        self.classifier.training = training\n",
    "        \n",
    "        n = tf.shape(inputs)[0]\n",
    "        \n",
    "        features = self.features(inputs)\n",
    "        features = tf.reshape(features, (n, -1))\n",
    "        \n",
    "        preds = self.classifier(features)\n",
    "        \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보시면, 의아해하실 수 있는게, 생성자에서 모델 구조를 두 개로 쪼개놨는데요, 이는 다음처럼 가운데에 flatten 레이어를 둬서 합칠 수도 있습니다.\n",
    "```python\n",
    "self.classifier = nn.Sequential([\n",
    "    # features 구조\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    \n",
    "    # classifier 구조\n",
    "])\n",
    "```\n",
    "\n",
    "그런데, 왜 저렇게 했냐면, 보통 transfer learning을 할때, convolution 부분만 떼오고 classifier는 내것으로 새로 구현하는 경우가 상당히 많죠. 그래서, 저는 convolution 파트와 dense 파트를 분리해서 구현하는 것을 좋아합니다.\n",
    "\n",
    "주의해야 할 점은 각 submodule(sequential 인 것들)의 첫 번째 레이어에 input_shape를 인자로 주시길 바랍니다.\n",
    "또한, 그래프를 빌드하고 빠른 속도로 돌리기 위해 call 메소드는 @tf.function으로 데코레이팅 해 주시길 바랍니다. propagation이 빠르답니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycnn = MyCNNModel()\n",
    "mycnn.build(input_shape=(None, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "빌드했으니, summary를 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_cnn_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential (Sequential)      (None, 7, 7, 12)          1036      \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 10)                40106     \n",
      "=================================================================\n",
      "Total params: 41,142\n",
      "Trainable params: 41,102\n",
      "Non-trainable params: 40\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mycnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자 이제, optimizer, criterion, metric을 정의해 볼 텐데요, 새로운 것을 추가해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam(learning_rate=1e-3)\n",
    "criterion = losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "train_acc = metrics.SparseCategoricalAccuracy()\n",
    "test_acc = metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "train_loss = metrics.Mean()\n",
    "test_loss = metrics.Mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "저 metric들은 compile할때 넣어주죠. 하지만, 저는 compile 및 fit을 사용하지 않을 것입니다. 좀 더 low level로 코딩할 것입니다.\n",
    "저 metric들이 편리한 이유는, 안에 값을 축적해 뒀다가 result()를 호출하면 한번에 결과를 뱉어줍니다. 우리가 1 epoch 다 돌때까지 히스토리를 추적할 필요가 사라지죠.\n",
    "원래, 배치 하나 돌고 loss 와 accuracy 저장하고, 이것을 1 epoch다 돌때까지 반복해서, 총 배치 개수로 나눠서 평균을 내주죠? 이것을 자동으로 해 준다고 생각하시면 됩니다.\n",
    "\n",
    "다음은, 필요한 변수를 선언해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은, forward propagation을 위한 함수로, @tf.function으로 데코레이팅 해 주었습니다. 이는, 일반적인 파이썬 함수를 텐서플로 그래프로 빌드할 수 있게 해 주며, 파이썬 코드로 돌릴때보다 속도를 더 빠르게 해 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def inference(model, criterion, x, y, training=False):\n",
    "    \"\"\"\n",
    "    Forward propagation 함수.\n",
    "    \n",
    "    Arguments:\n",
    "    ----------\n",
    "    model : tf.keras 모델 객체\n",
    "    criterion : loss 함수\n",
    "    x : 데이터 x\n",
    "    y : 라벨 y\n",
    "    training : True이면 loss, predicitons, gradient를 계산해서 리턴해주고, False이면 loss와 prediction만 계산해서 리턴해준다.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    loss : 이 배치에 대한 loss\n",
    "    preds : 이 배치에 대한 prediction 값\n",
    "    [grads] : trainig=True일때 반환되며, 그래디언트를 계산한 것\n",
    "    \"\"\"\n",
    "    \n",
    "    if training is True:\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = model(x)\n",
    "            loss = criterion(y, preds)\n",
    "            \n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        return loss, preds, grads\n",
    "    \n",
    "    else:\n",
    "        preds = model(x)\n",
    "        loss = criterion(y, preds)\n",
    "        return loss, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은, weight 업데이트 함수입니다. 역시 @tf.function으로 데코레이팅 해서 텐서플로 그래프로 만들어줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def backward(optimizer, grads, variables):\n",
    "    \"\"\"\n",
    "    Backward propagation 함수\n",
    "    \n",
    "    Arguments:\n",
    "    ----------\n",
    "    optimizer : optimizer 객체\n",
    "    grads : gradients list\n",
    "    variables : weights들의 list\n",
    "    \"\"\"\n",
    "    \n",
    "    optimizer.apply_gradients(zip(grads, variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제, 학습 루프를 구성해 봅시다. 사실 이 코드들은 fit()함수를 통해 간단하게 대체할 수 있지만, 나중에 복잡하고 커스텀 학습 방식을 구현할려면, fit을 사용하면 안됩니다.\n",
    "미리미리 fit을 사용하지 않고 학습 루프를 구현하는 연습을 해 둘 필요가 있죠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1/10\n",
      "Train loss: 0.37139958\n",
      "Train acc: 0.9010\n",
      "Test loss: 0.12006173\n",
      "Test acc: 0.9638\n",
      "\n",
      "Epochs: 2/10\n",
      "Train loss: 0.10331572\n",
      "Train acc: 0.9706\n",
      "Test loss: 0.06877670\n",
      "Test acc: 0.9781\n",
      "\n",
      "Epochs: 3/10\n",
      "Train loss: 0.07231963\n",
      "Train acc: 0.9790\n",
      "Test loss: 0.06409275\n",
      "Test acc: 0.9788\n",
      "\n",
      "Epochs: 4/10\n",
      "Train loss: 0.05619738\n",
      "Train acc: 0.9835\n",
      "Test loss: 0.05313132\n",
      "Test acc: 0.9835\n",
      "\n",
      "Epochs: 5/10\n",
      "Train loss: 0.04569481\n",
      "Train acc: 0.9868\n",
      "Test loss: 0.05213211\n",
      "Test acc: 0.9828\n",
      "\n",
      "Epochs: 6/10\n",
      "Train loss: 0.03941837\n",
      "Train acc: 0.9882\n",
      "Test loss: 0.04398380\n",
      "Test acc: 0.9864\n",
      "\n",
      "Epochs: 7/10\n",
      "Train loss: 0.03194543\n",
      "Train acc: 0.9909\n",
      "Test loss: 0.04153115\n",
      "Test acc: 0.9861\n",
      "\n",
      "Epochs: 8/10\n",
      "Train loss: 0.02816586\n",
      "Train acc: 0.9913\n",
      "Test loss: 0.04224466\n",
      "Test acc: 0.9850\n",
      "\n",
      "Epochs: 9/10\n",
      "Train loss: 0.02406918\n",
      "Train acc: 0.9928\n",
      "Test loss: 0.04003914\n",
      "Test acc: 0.9866\n",
      "\n",
      "Epochs: 10/10\n",
      "Train loss: 0.02154790\n",
      "Train acc: 0.9936\n",
      "Test loss: 0.04245353\n",
      "Test acc: 0.9856\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for e in range(EPOCHS):\n",
    "    x_shuffled, y_shuffled = shuffle(x_train, y_train)\n",
    "    \n",
    "    for x_batch, y_batch in next_batch(x_shuffled, y_shuffled, BATCH_SIZE):\n",
    "        loss, preds, grads = inference(mycnn, criterion, x_batch, y_batch, True)\n",
    "        \n",
    "        # 이것이, 결과를 축적해서 나중에 합쳐주는 metric입니다.\n",
    "        train_loss(loss)\n",
    "        train_acc(y_batch, preds)\n",
    "        \n",
    "        backward(optimizer, grads, mycnn.trainable_variables)\n",
    "        \n",
    "    for x_batch, y_batch in next_batch(x_test, y_test, BATCH_SIZE):\n",
    "        loss, preds = inference(mycnn, criterion, x_batch, y_batch, False)\n",
    "        \n",
    "        test_loss(loss)\n",
    "        test_acc(y_batch, preds)\n",
    "        \n",
    "    print(f\"Epochs: {e+1}/{EPOCHS}\")\n",
    "    print(f\"Train loss: {train_loss.result():.8f}\")\n",
    "    print(f\"Train acc: {train_acc.result():.4f}\")\n",
    "    print(f\"Test loss: {test_loss.result():.8f}\")\n",
    "    print(f\"Test acc: {test_acc.result():.4f}\")\n",
    "    print()\n",
    "    \n",
    "    # 축적한 놈들을 없애버립니다.\n",
    "    train_loss.reset_states()\n",
    "    train_acc.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_acc.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
